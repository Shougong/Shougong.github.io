<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shougong&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-01-10T03:55:05.680Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Shougong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer模型实现(二)</title>
    <link href="http://example.com/2021/01/10/Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E4%BA%8C/"/>
    <id>http://example.com/2021/01/10/Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E4%BA%8C/</id>
    <published>2021-01-10T03:42:15.000Z</published>
    <updated>2021-01-10T03:55:05.680Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;写在开头&quot;&gt;写在开头&lt;/h3&gt;
&lt;p&gt;前面一节复现了Transformer模型，但是不曾将其进行应用，这一篇主要用来记录自己使用Transformer模型进行文本分类的过程。&lt;/p&gt;
&lt;h3 id=&quot;前期准备&quot;&gt;前期准备&lt;/h3&gt;
&lt;p&gt;前面说过，Transformer模型由Encoder和Decoder构成，遵循Seq2Seq的结构模式，但是，想一下我们做分类的时候，通常是怎么做的呢？对于分类问题，我们通常使用模型学习数据的表示，比如使用BiLSTM模型，我们使用模型最后一个时间步的输出来作为输入样本的表示，这实际上是对样本进行了编码，然后送入一个全连接的神经网络进行分类，也就是说，对于分类问题，我们只需要得到输入句子的编码表示就可以了。回顾Transformer的架构，答案呼之欲出，我们只需要使用Transformer的encoder结构就可以支撑我们完成对样本的分类了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="transformer" scheme="http://example.com/tags/transformer/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch常用方法整理</title>
    <link href="http://example.com/2021/01/08/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86/"/>
    <id>http://example.com/2021/01/08/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86/</id>
    <published>2021-01-08T13:32:32.000Z</published>
    <updated>2021-01-09T09:18:43.547Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h3 id=&quot;arange-函数&quot;&gt;arange()函数&lt;/h3&gt;
&lt;p&gt;该函数与python中的range一样，生成一个tensor类型的行向量，也是左闭右开。&lt;/p&gt;
&lt;h4 id=&quot;参数：&quot;&gt;参数：&lt;/h4&gt;
&lt;p&gt;​		 </summary>
        
      
    
    
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Transformer模型实现(一)</title>
    <link href="http://example.com/2021/01/08/Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E4%B8%80/"/>
    <id>http://example.com/2021/01/08/Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E4%B8%80/</id>
    <published>2021-01-08T02:23:53.000Z</published>
    <updated>2021-01-09T02:13:02.039Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;写在开头&quot;&gt;写在开头&lt;/h3&gt;
&lt;p&gt;最近在做实验的时候，发现复现的一个模型在误分类上存在着attention过多考虑一些多次出现的词的情况，导致模型在捕捉关键特征上性能比较欠缺，实验后对模型进行分析，可能是引入concept后，因为attention是对整个concept构成的文本序列做的，不同entity对应的concept混合在一起后，会造成比较严重的歧义，合理的做法应当是让概念词仅作用于对应的entity，这样子会有更好的效果，此外，由于entity可能是多义的，比如&amp;quot;apple&amp;quot;，在现代语义中，有苹果（水果）和苹果公司两个含义在其中，因此需要做区分，直接使用attention可能忽视了原始文本中的上下文语义关系，最后做出误判，因此，选择合适的concept可能需要借助mask机制来实现，为此，记录下学习transformer实现的代码，本门的版本是借鉴了哈工大SCIR实验室翻译的哈佛大学的复现版本[1]。&lt;/p&gt;
&lt;h3 id=&quot;requirements&quot;&gt;requirements&lt;/h3&gt;
&lt;figure class=&quot;highlight txt&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;numpy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;matplotlib&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;spacy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;torchtext&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;seaborn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;python==3.7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;以上版本选择与当前环境兼容的即可。&lt;/p&gt;
&lt;p&gt;创建Transformer.py文件，引入上述库的模块。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn.functional &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; F&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; math, copy, time&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.autograd &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Variable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; seaborn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;seaborn.set_context(context=&lt;span class=&quot;string&quot;&gt;&amp;#x27;talk&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;模型简介&quot;&gt;模型简介&lt;/h3&gt;
&lt;img src=&quot;/2021/01/08/Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E4%B8%80/image-20210108104557302.png&quot; alt=&quot;image-20210108104557302&quot; style=&quot;zoom:80%;&quot;&gt;
&lt;p&gt;Figure 1是Transformer模型的整体架构图，遵循标准的Encoder-Decoder结构，这一架构的前身源于Seq2Seq + Att模型，首先看左边的Encoder部分，Encoder编码器由embedding层、位置编码（Positional Encoding）和N层（论文采用了6层）子层组成（特征提取层，这样的子层后来被叫做transformer层）。右边的Decoder解码器部分与Encoder部分在结构上比较接近，也是由embedding层、位置编码和N层子层组成。对于不同的应用，embedding层所使用的预训练向量是不同的，可能是但是这篇文章仅仅是介绍transformer模型的实现，因此，使用相同的向量空间。&lt;/p&gt;</summary>
    
    
    
    
    <category term="transformer" scheme="http://example.com/tags/transformer/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios</title>
    <link href="http://example.com/2020/12/17/A-Survey-on-Recent-Approaches-for-Natural-Language-Processing-in-Low-Resource-Scenarios/"/>
    <id>http://example.com/2020/12/17/A-Survey-on-Recent-Approaches-for-Natural-Language-Processing-in-Low-Resource-Scenarios/</id>
    <published>2020-12-17T11:48:43.000Z</published>
    <updated>2021-01-08T02:24:58.795Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这篇文章是最近才发表到arxiv上的，是对近期low-resource方向的一个总结。因为无论是短文本相关的分类还是现阶段nlp领域的关注方向，目前都着眼于解决数据或者特征稀疏情况下，如何保证优秀性能的问题，希望能对自己有一些启发。&lt;/p&gt;
&lt;p&gt;下图是文章对一些Low-Resource方法的调查总结。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/12/17/A-Survey-on-Recent-Approaches-for-Natural-Language-Processing-in-Low-Resource-Scenarios/image-20201217201950046.png&quot; alt=&quot;image-20201217201950046&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;low-resource定义&quot;&gt;Low-Resource定义&lt;/h3&gt;
&lt;p&gt;文章对Low-Resource进行了定义，即涵盖了低数据量的语言和某种特定领域的语言，在数据上具有低质量、数据集大小有限的特点，这样的情况称之为Low-Resource。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="low-resource" scheme="http://example.com/tags/low-resource/"/>
    
    <category term="survey" scheme="http://example.com/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</title>
    <link href="http://example.com/2020/12/10/Heterogeneous-Graph-Attention-Networks-for-Semi-supervised-Short-Text-Classification/"/>
    <id>http://example.com/2020/12/10/Heterogeneous-Graph-Attention-Networks-for-Semi-supervised-Short-Text-Classification/</id>
    <published>2020-12-10T00:59:15.000Z</published>
    <updated>2020-12-10T07:40:18.870Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这是19年发表在EMNLP上的文章，主要针对短文本分类做的相关工作。&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://www.aclweb.org/anthology/D19-1488/&quot;&gt;https://www.aclweb.org/anthology/D19-1488/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/ytc272098215/HGAT&quot;&gt;https://github.com/ytc272098215/HGAT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章主要立足于两个问题展开研究：异构图网络和半监督。&lt;/p&gt;
&lt;p&gt;使用半监督的原因是为了在很多实际场景下，标记数据通常是很少的，因此传统的监督学习方法很难有实际应用。&lt;/p&gt;
&lt;p&gt;异构图网络则是主要用来解决建图时，针对短文本的稀疏性所引入的不同类型的特征，并使用层次attention机制对不同特征分配不同的权重，以解决引入特征后所带来的噪声问题。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="GNNs" scheme="http://example.com/tags/GNNs/"/>
    
  </entry>
  
  <entry>
    <title>Graph Attention Networks</title>
    <link href="http://example.com/2020/12/09/Graph-Attention-Networks/"/>
    <id>http://example.com/2020/12/09/Graph-Attention-Networks/</id>
    <published>2020-12-09T02:03:45.000Z</published>
    <updated>2020-12-09T07:30:47.182Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这篇文章发表于ICLR 2018，属于现有技术在新模型上的一种应用。&lt;/p&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot;&gt;https://arxiv.org/abs/1710.10903&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;论文代码：&lt;a href=&quot;https://github.com/Diego999/pyGAT&quot;&gt;https://github.com/Diego999/pyGAT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章的核心是将attention机制与当前GNNs模型相结合，提出了GAT模型。那么文章是怎么做的attention的呢？（这部分将会在模型部分进行介绍）。&lt;/p&gt;
&lt;p&gt;GNNs系列模型大致分为两大类：基于频谱域和非频谱域。两类的区别在于，基于频谱域的方法将卷积操作定义在傅里叶域，这种方法需要计算图拉普拉斯矩阵的特征分解，这需要很大的计算开销，而且模型只能针对特定的图结构，因为不同的图结构对应的拉普拉斯矩阵的特征分解不同；而非频谱域方法则是通过聚合邻域节点的信息，直接在图上进行卷积操作，但是现有的方法无法适应不同邻域大小，通常通过对节点设定固定大小进行采样。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="GNNs" scheme="http://example.com/tags/GNNs/"/>
    
  </entry>
  
  <entry>
    <title>TextGCN复现</title>
    <link href="http://example.com/2020/12/07/TextGCN%E5%A4%8D%E7%8E%B0/"/>
    <id>http://example.com/2020/12/07/TextGCN%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-12-07T01:49:32.000Z</published>
    <updated>2020-12-07T05:26:22.273Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;本篇文章是对paper：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.05679&quot;&gt;Graph Convolutional Networks for Text Classification&lt;/a&gt;工作的复现，代码主要借鉴了&lt;a href=&quot;https://github.com/yao8839836/text_gcn&quot;&gt;论文原作者的&lt;/a&gt;和另一个&lt;a href=&quot;https://github.com/chengsen/PyTorch_TextGCN&quot;&gt;pytorch版本的&lt;/a&gt;，本文代码已经全部托管到我的&lt;a href=&quot;https://github.com/Shougong/TextGCN-pytorch&quot;&gt;github&lt;/a&gt;中。&lt;/p&gt;
&lt;p&gt;整个复现工作大致分为以下几个步骤：数据预处理、图构建、模型构建、训练和测试模型。&lt;/p&gt;
&lt;p&gt;采用的是原论文中使用的20NG数据集。&lt;/p&gt;
&lt;p&gt;最后效果与论文中的比较接近。&lt;/p&gt;
&lt;p&gt;实验结果：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;20NG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TextGCN&lt;/td&gt;
&lt;td&gt;0.8634&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TextGCN（ours）&lt;/td&gt;
&lt;td&gt;0.8533&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
    <category term="TextGCN" scheme="http://example.com/tags/TextGCN/"/>
    
  </entry>
  
  <entry>
    <title>KST-GCN A Knowledge-Driven Spatial-Temporal Graph Convolutional Network for Traffic Forecasting</title>
    <link href="http://example.com/2020/12/04/KST-GCN-A-Knowledge-Driven-Spatial-Temporal-Graph-Convolutional-Network-for-Traffic-Forecasting/"/>
    <id>http://example.com/2020/12/04/KST-GCN-A-Knowledge-Driven-Spatial-Temporal-Graph-Convolutional-Network-for-Traffic-Forecasting/</id>
    <published>2020-12-04T11:15:29.000Z</published>
    <updated>2020-12-07T08:36:01.331Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这是篇最近刚发表在arxiv上的文章，想学习下文章知识驱动的方法，此外，也想了解下在图像领域应用知识图谱的内容。&lt;/p&gt;
&lt;p&gt;这是一篇针对交通流预测的文章，文章认为，在考虑时空特征的交通条件下，引入外部知识，能提高交通流预测的准确率，但是现有的模型几乎没有考虑外部知识，或者在交通中忽视了这种复杂相关特征，这一点是文章的核心动机。&lt;/p&gt;
&lt;p&gt;但很显然的是，外部知识图谱与交通图显然是两个异构图，如何合理的融合这两种异构图是解决该知识驱动场景的一大关键。针对这一问题，文章提出来一种叫做KST-GCN的模型（在ST-GCN的基础上引入知识驱动）。具体做法就是，使用KS-Cells结合两种图上的信息，然后使用GRU来捕获交通图的时序状态。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="knowledge GCN" scheme="http://example.com/tags/knowledge-GCN/"/>
    
    <category term="Knowledge-Driven" scheme="http://example.com/tags/Knowledge-Driven/"/>
    
  </entry>
  
  <entry>
    <title>Efficient Strategies for Hierarchical Text Classification External Knowledge and Auxiliary Tasks</title>
    <link href="http://example.com/2020/12/04/Efficient-Strategies-for-Hierarchical-Text-Classification-External-Knowledge-and-Auxiliary-Tasks/"/>
    <id>http://example.com/2020/12/04/Efficient-Strategies-for-Hierarchical-Text-Classification-External-Knowledge-and-Auxiliary-Tasks/</id>
    <published>2020-12-04T00:53:39.000Z</published>
    <updated>2020-12-04T11:05:46.056Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这篇文章发表于ACL 2020，采用外部知识库和提出的一种辅助任务来做层次文本分类。因为之前一直接触的是单元标签类别，因此有些名词不太清楚，在这里先解释一下。&lt;/p&gt;
&lt;p&gt;文本分类从样本对应的标签数的角度来说，可以分成多元标签分类和单标签分类。例如一段文本可能既可以被划分为文具也可以被划分为日常用品，这就是典型的多分类，但是单标签则是一段文本只有一种对应分类，例如经济。在多元标签分类里面，有可以细分两种分类方向：hierarchical text classification和flat text classification，这两种的区别在于不同的标签之间是存在某种依赖关系的，例如：如一部电影可能是“喜剧片”，又是“爱情片”，而这电影的种类标签是平行的，没有层级结构；如一个电视产品，它属于“大家电”，也属于“家用电器”，而“大家电”标签是&amp;quot;家用电器&amp;quot;标签的子类，这产品所属种类标签是有层级结构，所以该类任务称为层级性多元标签分类。（摘自&lt;a href=&quot;https://zhuanlan.zhihu.com/p/108981524&quot;&gt;知乎&lt;/a&gt;）。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Topic Memory Networks for Short Text Classification</title>
    <link href="http://example.com/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/</id>
    <published>2020-11-30T06:18:56.000Z</published>
    <updated>2020-11-30T09:20:43.274Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;短文本分类因为数据稀疏特性，使得很多模型在该领域的性能表现不是很好（&lt;strong&gt;动机&lt;/strong&gt;）。以下Table 1为例：&lt;/p&gt;
&lt;img src=&quot;/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/image-20201130150631937.png&quot; alt=&quot;image-20201130150631937&quot; style=&quot;zoom:80%;&quot;&gt;
&lt;p&gt;从Table 1可以看到，在没有充分的上下文的情况下，模型倾向于将测试样例划分到R1，而不是R2，即模型倾向于把句子划分给句子结构相似的类别，这样子的误分类样本广泛存在于诸如CNN、LSTM之类的深度网络模型。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="topic model" scheme="http://example.com/tags/topic-model/"/>
    
  </entry>
  
  <entry>
    <title>Be More with Less, Hypergraph Attention Networks for Inductive Text Classification</title>
    <link href="http://example.com/2020/11/30/Be-More-with-Less-Hypergraph-Attention-Networks-for-Inductive-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Be-More-with-Less-Hypergraph-Attention-Networks-for-Inductive-Text-Classification/</id>
    <published>2020-11-30T06:08:33.000Z</published>
    <updated>2020-11-30T06:12:05.340Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这是发表于EMNLP 2020的一篇文章，主要对当前GNN模型存在的问题进行了改进。当前主流的GNN模型通常采用整个语料库中的单词对之间的共现特征作为图上节点的边关系，且在模型训练以前需要在整个语料库层面构建图结构，这带来了&lt;strong&gt;两个很严重的问题（文章的动机）：（i）模型无法捕获单词之间更高阶的交互特征；（ii）模型需要很大的计算开销&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;针对上述两个问题，文章受最近的&lt;strong&gt;超图神经网络&lt;/strong&gt;[1,2,3]的启发，利用对偶注意力机制，提出了一种归纳文本分类模型&lt;strong&gt;HyperGAT&lt;/strong&gt;，以此捕获单词之间的高阶交互特征，并降低模型计算复杂度。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="GNNs" scheme="http://example.com/tags/GNNs/"/>
    
  </entry>
  
  <entry>
    <title>Cognitive Aspects-Based Short Text Representation with Named Entity, Concept and Knowledge</title>
    <link href="http://example.com/2020/11/30/Cognitive-Aspects-Based-Short-Text-Representation-with-Named-Entity-Concept-and-Knowledge/"/>
    <id>http://example.com/2020/11/30/Cognitive-Aspects-Based-Short-Text-Representation-with-Named-Entity-Concept-and-Knowledge/</id>
    <published>2020-11-30T06:02:51.000Z</published>
    <updated>2020-11-30T06:04:47.218Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;短文本缺少足够的共同出现的单词和共享的上下文信息，具有稀疏和简短的特点，导致短文本分类问题效果不佳。因为单词、实体、概念和知识库中的实体对短文本分类具有不同的认知信息，现有的解决方法基于这些内容，从文本认知角度来丰富短文本的表示，通常包含三个方面：（1）丰富语义概念；（2）引入知识；（3）引入文本类别。这一角度的本质是增强短文本表示的语义，该方式有以下好处：（1）从知识库中挖掘实体关系可以增强短文本语义表示。（2）实体级别的表示形式可以帮助消除具有相同拼写的术语的歧义。（3）与单词和实体表示形式相比，概念表示形式更为抽象，且是基于关键字和实体的短文本表示的重要补充。但是先前的研究例如KPCNN和STCKA模型并没有充分利用知识库中的信息，&lt;strong&gt;仅考虑了知识库中的单个方面（实体或概念信息）&lt;/strong&gt;。针对这一问题，本文提出了ECKA模型。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="Knowledge power" scheme="http://example.com/tags/Knowledge-power/"/>
    
  </entry>
  
  <entry>
    <title>Deep Short Text Classification with Knowledge Powered Attention</title>
    <link href="http://example.com/2020/11/30/Deep-Short-Text-Classification-with-Knowledge-Powered-Attention/"/>
    <id>http://example.com/2020/11/30/Deep-Short-Text-Classification-with-Knowledge-Powered-Attention/</id>
    <published>2020-11-30T05:58:19.000Z</published>
    <updated>2020-12-04T07:30:07.125Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;
&lt;p&gt;这篇文章是AAAI 2019的论文，属于知识图谱在下游任务的应用。短文本分类一直是文档分类任务中比较难的一个任务，因为短文本缺乏足够的上下文信息，与段落和文档相比，不易分类。例如：&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&quot;Jay grew up in Taiwan.&quot;&lt;/p&gt;
&lt;p&gt;上述文本中，&amp;quot;Jay&amp;quot;表示周杰伦，这是个人名，传统的文本分类模型会把这个视为一个新的词，这样子就会忽视&amp;quot;Jay&amp;quot;是一个歌手这一信息，这个信息会有助于将文本分类到娱乐类别，但是忽视这一信息就会导致模型将其误分类为错误的类别。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="Knowledge power" scheme="http://example.com/tags/Knowledge-power/"/>
    
  </entry>
  
  <entry>
    <title>Graph Convolutional Networks for Text Classification</title>
    <link href="http://example.com/2020/11/30/Graph-Convolutional-Networks-for-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Graph-Convolutional-Networks-for-Text-Classification/</id>
    <published>2020-11-30T05:51:33.000Z</published>
    <updated>2020-11-30T05:56:23.838Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;TextCNN、RNN等模型在NLP领域应用广泛，但是这两种深度网络捕获的是局部连续单词序列中的语义和句法信息，会忽略掉非连续的、长距离的语义特征（全局特征），这也限制了模型性能的上限。文章使用了最近兴起的GCN模型解决文本分类问题，图神经网络（GNN）既能有效地捕获节点之间的丰富关系，也能保留图中的全局结构信息，这对改善CNN和RNN模型难以捕获全局特征带来了希望。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="GCN" scheme="http://example.com/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>北京记忆</title>
    <link href="http://example.com/2020/11/30/%E5%8C%97%E4%BA%AC%E8%AE%B0%E5%BF%86/"/>
    <id>http://example.com/2020/11/30/%E5%8C%97%E4%BA%AC%E8%AE%B0%E5%BF%86/</id>
    <published>2020-11-30T04:50:12.000Z</published>
    <updated>2020-11-30T04:52:10.289Z</updated>
    
    
    
    
    <category term="旅行" scheme="http://example.com/categories/%E6%97%85%E8%A1%8C/"/>
    
    
    <category term="旅行" scheme="http://example.com/tags/%E6%97%85%E8%A1%8C/"/>
    
  </entry>
  
  <entry>
    <title>Text Classification Using Label Names Only, A Language Model Self-Training Approach</title>
    <link href="http://example.com/2020/11/29/Text-Classification-Using-Label-Names-Only-A-Language-Model-Self-Training-Approach/"/>
    <id>http://example.com/2020/11/29/Text-Classification-Using-Label-Names-Only-A-Language-Model-Self-Training-Approach/</id>
    <published>2020-11-29T15:25:26.000Z</published>
    <updated>2020-11-30T03:13:03.264Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;现阶段文本分类方法需要基于大量的人为标注数据进行训练，这种方法缺乏实际应用性不说，而人类只需要基于描述待分类类别的少数单词就可以准确分类，因为人类对这些标签具有先验知识，能理解这些标签。&lt;/p&gt;
&lt;p&gt;当前文本分类方向较为前沿的是&lt;strong&gt;半监督文本分类方法&lt;/strong&gt;和&lt;strong&gt;零次学习方法&lt;/strong&gt;。半监督学习方法主要分为两种途径：&lt;strong&gt;基于增强学习的方法&lt;/strong&gt;和&lt;strong&gt;基于图的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增强学习方法&lt;/strong&gt;通常是通过产生新的例子，并对模型的预测进行正则化以范化模型性能，增强实例的方法一般采用通过反向翻译创建为真实文本序列、通过扰动或插值在模型的隐藏状态下创建。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-2.10.1+ZooKeeper-3.6.2+Hbase-2.3.2+Hive-2.3.7安装</title>
    <link href="http://example.com/2020/11/28/Hadoop-2-10-1-ZooKeeper-3-6-2-Hbase-2-3-2-Hive-2-3-7%E5%AE%89%E8%A3%85/"/>
    <id>http://example.com/2020/11/28/Hadoop-2-10-1-ZooKeeper-3-6-2-Hbase-2-3-2-Hive-2-3-7%E5%AE%89%E8%A3%85/</id>
    <published>2020-11-28T06:59:55.000Z</published>
    <updated>2020-11-30T09:24:33.905Z</updated>
    
    
    <summary type="html">&lt;p&gt;记录Hadoop-2.10.1+ZooKeeper-3.6.2+Hbase-2.3.2+Hive-2.3.7安装的一次经历。&lt;/p&gt;
&lt;h3 id=&quot;预安装jdk1-8-0&quot;&gt;预安装jdk1.8.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从https://www.oracle.com/java/technologies/javase-downloads.html下载jdk-8u261-linux-x64.rpm，该安装包已经下载好了，并存于安装目录下。&lt;/li&gt;
&lt;li&gt;如果本机上有别的java版本，可以采用两种方式，删除原先版本或者不安装该版本。&lt;/li&gt;
&lt;li&gt;设置安装包的权限：chmod +x jdk-8u261-linux-x64.rpm&lt;/li&gt;
&lt;li&gt;rpm安装jdk1.8.0_u261：rpm -ivh jdk-8u261-linux-x64.rpm&lt;/li&gt;
&lt;li&gt;在命令行输入以下命令来配置java路径：&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    <category term="分布式" scheme="http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
    <category term="hadoop" scheme="http://example.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2020/11/28/hello-world/"/>
    <id>http://example.com/2020/11/28/hello-world/</id>
    <published>2020-11-28T03:00:42.467Z</published>
    <updated>2020-11-30T04:51:48.648Z</updated>
    
    
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
