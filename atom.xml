<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shougong&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-12-07T05:26:03.188Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Shougong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TextGCN复现</title>
    <link href="http://example.com/2020/12/07/TextGCN%E5%A4%8D%E7%8E%B0/"/>
    <id>http://example.com/2020/12/07/TextGCN%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-12-07T01:49:32.000Z</published>
    <updated>2020-12-07T05:26:03.188Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;本篇文章是对paper：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.05679&quot;&gt;Graph Convolutional Networks for Text Classification&lt;/a&gt;工作的复现，代码主要借鉴了&lt;a href=&quot;https://github.com/yao8839836/text_gcn&quot;&gt;论文原作者的&lt;/a&gt;和另一个&lt;a href=&quot;https://github.com/chengsen/PyTorch_TextGCN&quot;&gt;pytorch版本的&lt;/a&gt;，本文代码已经全部托管到我的&lt;a href=&quot;https://github.com/Shougong/TextGCN-pytorch&quot;&gt;github&lt;/a&gt;中。&lt;/p&gt;
&lt;p&gt;整个复现工作大致分为以下几个步骤：数据预处理、图构建、模型构建、训练和测试模型。&lt;/p&gt;
&lt;p&gt;采用的是原论文中使用的20NG数据集。&lt;/p&gt;
&lt;p&gt;最后效果与论文中的比较接近。&lt;/p&gt;
&lt;p&gt;实验结果：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;20NG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TextGCN&lt;/td&gt;
&lt;td&gt;0.8634&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TextGCN（ours）&lt;/td&gt;
&lt;td&gt;0.8533&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="TextGCN" scheme="http://example.com/tags/TextGCN/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>KST-GCN A Knowledge-Driven Spatial-Temporal Graph Convolutional Network for Traffic Forecasting</title>
    <link href="http://example.com/2020/12/04/KST-GCN-A-Knowledge-Driven-Spatial-Temporal-Graph-Convolutional-Network-for-Traffic-Forecasting/"/>
    <id>http://example.com/2020/12/04/KST-GCN-A-Knowledge-Driven-Spatial-Temporal-Graph-Convolutional-Network-for-Traffic-Forecasting/</id>
    <published>2020-12-04T11:15:29.000Z</published>
    <updated>2020-12-07T01:49:40.384Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这是篇最近刚发表在arxiv上的文章，想学习下文章知识驱动的方法，此外，也想了解下在图像领域应用知识图谱的内容。&lt;/p&gt;
&lt;p&gt;这是一篇针对交通流预测的文章，文章认为，在考虑时空特征的交通条件下，引入外部知识，能提高交通流预测的准确率，但是现有的模型几乎没有考虑外部知识，或者在交通中忽视了这种复杂相关特征，这一点是文章的核心动机。&lt;/p&gt;
&lt;p&gt;但很显然的是，外部知识图谱与交通图显然是两个异构图，如何合理的融合这两种异构图是解决该知识驱动场景的一大关键。针对这一问题，文章提出来一种叫做KST-GCN的模型（在ST-GCN的基础上引入知识驱动）。具体做法就是，使用KS-Cells结合两种图上的信息，然后使用GRU来捕获交通图的时序状态。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="knowledge GCN" scheme="http://example.com/tags/knowledge-GCN/"/>
    
    <category term="Knowledge-Driven" scheme="http://example.com/tags/Knowledge-Driven/"/>
    
  </entry>
  
  <entry>
    <title>Efficient Strategies for Hierarchical Text Classification External Knowledge and Auxiliary Tasks</title>
    <link href="http://example.com/2020/12/04/Efficient-Strategies-for-Hierarchical-Text-Classification-External-Knowledge-and-Auxiliary-Tasks/"/>
    <id>http://example.com/2020/12/04/Efficient-Strategies-for-Hierarchical-Text-Classification-External-Knowledge-and-Auxiliary-Tasks/</id>
    <published>2020-12-04T00:53:39.000Z</published>
    <updated>2020-12-04T11:05:46.056Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这篇文章发表于ACL 2020，采用外部知识库和提出的一种辅助任务来做层次文本分类。因为之前一直接触的是单元标签类别，因此有些名词不太清楚，在这里先解释一下。&lt;/p&gt;
&lt;p&gt;文本分类从样本对应的标签数的角度来说，可以分成多元标签分类和单标签分类。例如一段文本可能既可以被划分为文具也可以被划分为日常用品，这就是典型的多分类，但是单标签则是一段文本只有一种对应分类，例如经济。在多元标签分类里面，有可以细分两种分类方向：hierarchical text classification和flat text classification，这两种的区别在于不同的标签之间是存在某种依赖关系的，例如：如一部电影可能是“喜剧片”，又是“爱情片”，而这电影的种类标签是平行的，没有层级结构；如一个电视产品，它属于“大家电”，也属于“家用电器”，而“大家电”标签是&amp;quot;家用电器&amp;quot;标签的子类，这产品所属种类标签是有层级结构，所以该类任务称为层级性多元标签分类。（摘自&lt;a href=&quot;https://zhuanlan.zhihu.com/p/108981524&quot;&gt;知乎&lt;/a&gt;）。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Topic Memory Networks for Short Text Classification</title>
    <link href="http://example.com/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/</id>
    <published>2020-11-30T06:18:56.000Z</published>
    <updated>2020-11-30T09:20:43.274Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;短文本分类因为数据稀疏特性，使得很多模型在该领域的性能表现不是很好（&lt;strong&gt;动机&lt;/strong&gt;）。以下Table 1为例：&lt;/p&gt;
&lt;img src=&quot;/2020/11/30/Topic-Memory-Networks-for-Short-Text-Classification/image-20201130150631937.png&quot; alt=&quot;image-20201130150631937&quot; style=&quot;zoom:80%;&quot;&gt;
&lt;p&gt;从Table 1可以看到，在没有充分的上下文的情况下，模型倾向于将测试样例划分到R1，而不是R2，即模型倾向于把句子划分给句子结构相似的类别，这样子的误分类样本广泛存在于诸如CNN、LSTM之类的深度网络模型。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="topic model" scheme="http://example.com/tags/topic-model/"/>
    
  </entry>
  
  <entry>
    <title>Be More with Less, Hypergraph Attention Networks for Inductive Text Classification</title>
    <link href="http://example.com/2020/11/30/Be-More-with-Less-Hypergraph-Attention-Networks-for-Inductive-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Be-More-with-Less-Hypergraph-Attention-Networks-for-Inductive-Text-Classification/</id>
    <published>2020-11-30T06:08:33.000Z</published>
    <updated>2020-11-30T06:12:05.340Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;这是发表于EMNLP 2020的一篇文章，主要对当前GNN模型存在的问题进行了改进。当前主流的GNN模型通常采用整个语料库中的单词对之间的共现特征作为图上节点的边关系，且在模型训练以前需要在整个语料库层面构建图结构，这带来了&lt;strong&gt;两个很严重的问题（文章的动机）：（i）模型无法捕获单词之间更高阶的交互特征；（ii）模型需要很大的计算开销&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;针对上述两个问题，文章受最近的&lt;strong&gt;超图神经网络&lt;/strong&gt;[1,2,3]的启发，利用对偶注意力机制，提出了一种归纳文本分类模型&lt;strong&gt;HyperGAT&lt;/strong&gt;，以此捕获单词之间的高阶交互特征，并降低模型计算复杂度。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="GNNs" scheme="http://example.com/tags/GNNs/"/>
    
  </entry>
  
  <entry>
    <title>Cognitive Aspects-Based Short Text Representation with Named Entity, Concept and Knowledge</title>
    <link href="http://example.com/2020/11/30/Cognitive-Aspects-Based-Short-Text-Representation-with-Named-Entity-Concept-and-Knowledge/"/>
    <id>http://example.com/2020/11/30/Cognitive-Aspects-Based-Short-Text-Representation-with-Named-Entity-Concept-and-Knowledge/</id>
    <published>2020-11-30T06:02:51.000Z</published>
    <updated>2020-11-30T06:04:47.218Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;短文本缺少足够的共同出现的单词和共享的上下文信息，具有稀疏和简短的特点，导致短文本分类问题效果不佳。因为单词、实体、概念和知识库中的实体对短文本分类具有不同的认知信息，现有的解决方法基于这些内容，从文本认知角度来丰富短文本的表示，通常包含三个方面：（1）丰富语义概念；（2）引入知识；（3）引入文本类别。这一角度的本质是增强短文本表示的语义，该方式有以下好处：（1）从知识库中挖掘实体关系可以增强短文本语义表示。（2）实体级别的表示形式可以帮助消除具有相同拼写的术语的歧义。（3）与单词和实体表示形式相比，概念表示形式更为抽象，且是基于关键字和实体的短文本表示的重要补充。但是先前的研究例如KPCNN和STCKA模型并没有充分利用知识库中的信息，&lt;strong&gt;仅考虑了知识库中的单个方面（实体或概念信息）&lt;/strong&gt;。针对这一问题，本文提出了ECKA模型。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="Knowledge power" scheme="http://example.com/tags/Knowledge-power/"/>
    
  </entry>
  
  <entry>
    <title>Deep Short Text Classification with Knowledge Powered Attention</title>
    <link href="http://example.com/2020/11/30/Deep-Short-Text-Classification-with-Knowledge-Powered-Attention/"/>
    <id>http://example.com/2020/11/30/Deep-Short-Text-Classification-with-Knowledge-Powered-Attention/</id>
    <published>2020-11-30T05:58:19.000Z</published>
    <updated>2020-12-04T07:30:07.125Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;
&lt;p&gt;这篇文章是AAAI 2019的论文，属于知识图谱在下游任务的应用。短文本分类一直是文档分类任务中比较难的一个任务，因为短文本缺乏足够的上下文信息，与段落和文档相比，不易分类。例如：&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&quot;Jay grew up in Taiwan.&quot;&lt;/p&gt;
&lt;p&gt;上述文本中，&amp;quot;Jay&amp;quot;表示周杰伦，这是个人名，传统的文本分类模型会把这个视为一个新的词，这样子就会忽视&amp;quot;Jay&amp;quot;是一个歌手这一信息，这个信息会有助于将文本分类到娱乐类别，但是忽视这一信息就会导致模型将其误分类为错误的类别。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="Knowledge power" scheme="http://example.com/tags/Knowledge-power/"/>
    
  </entry>
  
  <entry>
    <title>Graph Convolutional Networks for Text Classification</title>
    <link href="http://example.com/2020/11/30/Graph-Convolutional-Networks-for-Text-Classification/"/>
    <id>http://example.com/2020/11/30/Graph-Convolutional-Networks-for-Text-Classification/</id>
    <published>2020-11-30T05:51:33.000Z</published>
    <updated>2020-11-30T05:56:23.838Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;TextCNN、RNN等模型在NLP领域应用广泛，但是这两种深度网络捕获的是局部连续单词序列中的语义和句法信息，会忽略掉非连续的、长距离的语义特征（全局特征），这也限制了模型性能的上限。文章使用了最近兴起的GCN模型解决文本分类问题，图神经网络（GNN）既能有效地捕获节点之间的丰富关系，也能保留图中的全局结构信息，这对改善CNN和RNN模型难以捕获全局特征带来了希望。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
    <category term="GCN" scheme="http://example.com/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>北京记忆</title>
    <link href="http://example.com/2020/11/30/%E5%8C%97%E4%BA%AC%E8%AE%B0%E5%BF%86/"/>
    <id>http://example.com/2020/11/30/%E5%8C%97%E4%BA%AC%E8%AE%B0%E5%BF%86/</id>
    <published>2020-11-30T04:50:12.000Z</published>
    <updated>2020-11-30T04:52:10.289Z</updated>
    
    
    
    
    <category term="旅行" scheme="http://example.com/categories/%E6%97%85%E8%A1%8C/"/>
    
    
    <category term="旅行" scheme="http://example.com/tags/%E6%97%85%E8%A1%8C/"/>
    
  </entry>
  
  <entry>
    <title>Text Classification Using Label Names Only, A Language Model Self-Training Approach</title>
    <link href="http://example.com/2020/11/29/Text-Classification-Using-Label-Names-Only-A-Language-Model-Self-Training-Approach/"/>
    <id>http://example.com/2020/11/29/Text-Classification-Using-Label-Names-Only-A-Language-Model-Self-Training-Approach/</id>
    <published>2020-11-29T15:25:26.000Z</published>
    <updated>2020-11-30T03:13:03.264Z</updated>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;现阶段文本分类方法需要基于大量的人为标注数据进行训练，这种方法缺乏实际应用性不说，而人类只需要基于描述待分类类别的少数单词就可以准确分类，因为人类对这些标签具有先验知识，能理解这些标签。&lt;/p&gt;
&lt;p&gt;当前文本分类方向较为前沿的是&lt;strong&gt;半监督文本分类方法&lt;/strong&gt;和&lt;strong&gt;零次学习方法&lt;/strong&gt;。半监督学习方法主要分为两种途径：&lt;strong&gt;基于增强学习的方法&lt;/strong&gt;和&lt;strong&gt;基于图的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增强学习方法&lt;/strong&gt;通常是通过产生新的例子，并对模型的预测进行正则化以范化模型性能，增强实例的方法一般采用通过反向翻译创建为真实文本序列、通过扰动或插值在模型的隐藏状态下创建。&lt;/p&gt;</summary>
    
    
    
    <category term="paper notes" scheme="http://example.com/categories/paper-notes/"/>
    
    
    <category term="text classification" scheme="http://example.com/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-2.10.1+ZooKeeper-3.6.2+Hbase-2.3.2+Hive-2.3.7安装</title>
    <link href="http://example.com/2020/11/28/Hadoop-2-10-1-ZooKeeper-3-6-2-Hbase-2-3-2-Hive-2-3-7%E5%AE%89%E8%A3%85/"/>
    <id>http://example.com/2020/11/28/Hadoop-2-10-1-ZooKeeper-3-6-2-Hbase-2-3-2-Hive-2-3-7%E5%AE%89%E8%A3%85/</id>
    <published>2020-11-28T06:59:55.000Z</published>
    <updated>2020-11-30T09:24:33.905Z</updated>
    
    
    <summary type="html">&lt;p&gt;记录Hadoop-2.10.1+ZooKeeper-3.6.2+Hbase-2.3.2+Hive-2.3.7安装的一次经历。&lt;/p&gt;
&lt;h3 id=&quot;预安装jdk1-8-0&quot;&gt;预安装jdk1.8.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从https://www.oracle.com/java/technologies/javase-downloads.html下载jdk-8u261-linux-x64.rpm，该安装包已经下载好了，并存于安装目录下。&lt;/li&gt;
&lt;li&gt;如果本机上有别的java版本，可以采用两种方式，删除原先版本或者不安装该版本。&lt;/li&gt;
&lt;li&gt;设置安装包的权限：chmod +x jdk-8u261-linux-x64.rpm&lt;/li&gt;
&lt;li&gt;rpm安装jdk1.8.0_u261：rpm -ivh jdk-8u261-linux-x64.rpm&lt;/li&gt;
&lt;li&gt;在命令行输入以下命令来配置java路径：&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    <category term="分布式" scheme="http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
    <category term="hadoop" scheme="http://example.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2020/11/28/hello-world/"/>
    <id>http://example.com/2020/11/28/hello-world/</id>
    <published>2020-11-28T03:00:42.467Z</published>
    <updated>2020-11-30T04:51:48.648Z</updated>
    
    
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
